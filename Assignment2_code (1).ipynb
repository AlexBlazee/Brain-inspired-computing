{"cells":[{"cell_type":"markdown","metadata":{"id":"mArpPMTEhgw9"},"source":["# Learning Goals\n","\n","In Assignment 1, we studied how information is represented by a single spiking neuron. In this assignment, you will learn how to construct networks of spiking neurons for a given cognitive task, how to propagate information through a network, and understanding the intuition behind network design choices.Â \n","\n","Let's first import all the libraries required for this assignment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EK4XYK9hgw_"},"outputs":[],"source":["import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"1m1yBa9ohgxA"},"source":["# Question 1: From single neuron to network of neurons [15 points]\n","## 1a.\n","What computational advantages do networks of neurons offer when compared against information processing by a single neuron? In other words, why do we need networks of neurons? "]},{"cell_type":"markdown","metadata":{"id":"fdF4M2C1hgxA"},"source":["## Answer 1a.\n","\n","The network of neurons offer significant improvements over single neurons like  : \n","\n","**Distributed processing** :  Each individual neuron has a limited computational capability but when they work together in a network , each one can brings its unique perspective to a complex problem allowing for more accuracy.\n","\n","**Adaptability** :  A single neuron's behavior is rigid and cannot be modified. In contrast network of neurons can adapt and better understand the environment by modifying connections(strengthening of good connections and weakening of bad connection aka updating weights).\n","\n","**Parallel processing** : A network of neurons can process multiple inputs simultaneously enabling faster computation and handling of larger inputs, unlike a single neuron that can only process one input at a time.\n","\n","**Non-linearity** : The combination of many nonlinear neurons can create complex nonlinear functions that can model complex systems.\n","\n","**Robustness** : Networks of neurons are more robust to noise and errors in the input. A single neuron may be affected by small errors, but a network of neurons can still provide accurate results despite small errors.\n","\n","**Fault tolerance** : Networks of neurons can continue to function even if some neurons fail. In contrast, a single neuron failure can lead to the complete failure of the system.\n"]},{"cell_type":"markdown","metadata":{"id":"weg3PWu9hgxA"},"source":["## 1b. \n","Describe the algorithm for the information flow through a network of spiking leaky-integrate-and-fire (LIF) neurons. \n","\n","Specifically, trace out the steps required to compute network output from a given (continuous-valued) inputs (suppose it is the raw mnist image in assignment 1). \n","\n","The algorithm should describe  \n","\n","Step (1) Encoding: how continuous-valued inputs are fed to the SNN input layer; \n","\n","Step (2) Processing: how the layer activations are computed; \n","\n","Step (3) Decoding: and how the output layer activity is decoded and used for downstream tasks (say if we want to use the output for some image classification task). \n","\n","Also, provide a diagrammatic overview of the algorithm to aid your explanation. \n","\n","You are free to assume any network size, and input and output dimensions. "]},{"cell_type":"markdown","metadata":{"id":"pWxqsqGlhgxB"},"source":["## Answer 1b.\n","\n","**Encoding :**  \n","\n","The continuous-valued inputs (such as the raw MNIST image in assignment 1) are first encoded into spike trains that can be processed by the networks. \n","This involves mapping the input values to the firing rate of a group of input neurons, where the firing rate represents the intensity of the input.To achieve this, the input layer of the network consists of a set of neurons, each of which receives a particular input feature.The activity of each neuron in the input layer is determined by the corresponding feature value in the input vector.\n","\n","For example, in the case of the MNIST image classification task, each pixel in the input image corresponds to an input feature, and the activity of each neuron in the input layer is proportional to the pixel value. This activity is then converted to a spike rate using a spike rate encoding function, such as the Poisson rate coding.\n","\n","The input layer is typically connected to the first hidden layer through a fully connected weight matrix.\n","\n","**Processing :**\n","\n","The layer activations are computed by simulating the spiking behavior of the LIF neurons in the network. The LIF neuron model consists of three components: an input current, a membrane potential, and a spiking threshold. The input current is proportional to the firing rate of the input neurons, and it is integrated over time to update the membrane potential of the LIF neuron. Once the membrane potential reaches the spiking threshold, the LIF neuron generates a spike and its membrane potential is reset. The spike is then transmitted to the next layer of the network.\n","\n","The activations of the hidden layer are computed by receiving and processing the spike trains from the input layer.\n","\n","The neurons in the hidden layer integrate the incoming spikes over time, with each neuron having a specific set of synaptic weights that determine the strength of the connections between the input neurons and the hidden layer neurons.\n","Once the membrane potential of a neuron exceeds a threshold, the neuron spikes and sends an output signal to the next layer and its membrane potential is reset.\n","\n","The weights of the connections between neurons determine the strength of the synaptic transmission, which affects the integration of incoming spikes.\n","\n","\n","**Decoding :**\n","\n","The final step in the algorithm is to decode the output activity of the network to obtain a useful output signal. This involves mapping the spiking activity of the output neurons to a continuous-valued output signal.\n","\n","In the case of the MNIST image classification task, the output layer of the network consists of a set of neurons, each of which corresponds to a particular digit class. The activity of each neuron in the output layer reflects the degree of confidence that the network has in assigning the input image to the corresponding class.\n","\n","To obtain the final output, a decoding mechanism such as winner-takes-all or softmax is used to determine the most active neuron in the output layer, which corresponds to the predicted class label for the input image.\n","\n","Diagramatic Overview\n","\n","                    [ Continuous-valued inputs ]\n","                                |\n","                                V\n","                [ Input layer - Poisson spike train encoding ]\n","                                |\n","                                V\n","                       [ Spiking LIF neurons ]\n","                                |\n","                                V\n","                       [ Spike Propagation ]\n","                                |\n","                                V\n","                      [ Synaptic integration ]\n","                                |\n","                                V\n","                        [ Spike output ]\n","                                |\n","                                V\n","        [ Output spiking activity spike decoding winner-takes-all, softmax) ]\n","                                |\n","                                V\n","                        [ Final output ]\n","\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"KDvz8tkHhgxB"},"source":["# Question 2: Elements of Constructing Feedforward Networks [20 points]\n","In this exercise, you will implement the two fundamental components of a feedforward spiking neural network: i) layers of neurons and ii) connections between those layers\n","## 2a. \n","As the first step towards creating an SNN, we will create a class that defines a layer of LIF neurons. The layer object creates a collection of LIF neurons and applies input current to it (also called psp_input for postsynaptic input) to produce the collective spiking output of the layer. \n","\n","Below is the class definition for a layer of LIFNeurons. Fill in the components to define the layer. \n","\n","## Rubric: 10 points for correct implementation of class. 5 points for the right verification. [15 points]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBZmMYCmhgxB"},"outputs":[],"source":["class LIFNeurons:\n","    \"\"\" Define Leaky Integrate-and-Fire Neuron Layer \"\"\"\n","\n","    def __init__(self, dimension, vdecay, vth):\n","        \"\"\"\n","        Args:\n","            dimension (int): Number of LIF neurons in the layer\n","            vdecay (float): voltage decay of LIF neurons\n","            vth (float): voltage threshold of LIF neurons\n","        \n","        This function is complete. You do not need to do anything here.\n","        \"\"\"\n","        self.dimension = dimension\n","        self.vdecay = vdecay\n","        self.vth = vth\n","\n","        # Initialize LIF neuron states\n","        self.voltage = np.zeros(self.dimension)\n","#        self.current = np.zeros(self.dimension)\n","        self.spike = np.zeros(self.dimension)\n","#        self.cdecay = 0.5\n","#        self.vrest = 0\n","    \n","    def __call__(self, psp_input):\n","        \"\"\"\n","        Args:\n","            psp_input (ndarray): synaptic input current at a single timestep. The shape of this is same as the number of neurons in the layer. \n","        Return:\n","            self.spike: output spikes from the layer. The shape of this should be the same as the number of neurons in the layer. \n","        \n","        Write the expressions for updating the voltage and generating the spikes for the layer given psp_input at one timestep. \n","        \"\"\"\n","        #Update the voltage\n","        #Generate the spikes from the voltage         \n","        #Reset the voltage if the neuron spikes\n","\n","#         self.voltage[0] = psp_input[0]\n","#         if self.voltage[0] >= self.vth:\n","#           self.voltage[0] = self.vrest\n","#           self.spike[0] =1 \n","\n","#         for t in range(1,self.dimension):    \n","#           self.current[t] = self.current[t-1]*self.cdecay + psp_input[t]\n","#           self.voltage[t] = self.voltage[t-1]*self.vdecay + self.current[t]\n","#           if self.voltage[t] >= self.vth:\n","#             self.voltage[t] = self.vrest\n","#             self.spike[t] =1 \n","\n","        self.voltage = self.voltage * self.vdecay * (1 - self.spike) + psp_input\n","        self.spike = (self.voltage > self.vth).astype(float)\n","        \n","        return self.spike"]},{"cell_type":"markdown","metadata":{"id":"k8b0Bk_yhgxC"},"source":["To verify the correctness of your class implementation, create a layer of neurons using the class definition above, and pass through it random inputs. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DST01dZJhgxC","outputId":"1abc9631-2188-4680-e115-314c0eb5c960"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Spikes :  [1 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0]\n","Output spikes :  [1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0.]\n"]}],"source":["#Create a layer of neurons using the class definition above\n","model_LIF = LIFNeurons( dimension = 20 ,vdecay = 0.5 , vth = 0.5)\n","#Create random input spikes with any probability and print them. Numpy random.choice function might be useful here. \n","input_spikes = np.random.choice([0,1],20 , replace = True, p=[0.5,0.5])\n","print(\"Input Spikes : \", input_spikes)\n","#Propagate the random input spikes through the layer and print the output\n","\n","output_spikes = model_LIF.__call__(input_spikes)\n","print(\"Output spikes : \",output_spikes )"]},{"cell_type":"markdown","metadata":{"id":"LyUlFC9ehgxC"},"source":["## 2b.\n","Now, we will create a class the defines the connection between a presynaptic layer and a postsynaptic layer. To create the connection, we need the activity of the presynaptic layer (also called presynaptic layer activation) and the weight matrix connecting the presynaptic and postsynaptic neurons. The output of the class should be the current for the postsynaptic layer. \n","\n","Below is the class definition for Connections. Fill in the components to create the connections. \n","\n","## Rubric: 3 points for correct implementation of class. 2 points for the right verification. [5 points]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3lrZkREhgxC"},"outputs":[],"source":["class Connections:\n","    \"\"\" Define connections between spiking neuron layers \"\"\"\n","\n","    def __init__(self, weights, pre_dimension, post_dimension):\n","        \"\"\"\n","        Args:\n","            pre_dimension (int): number of neurons in the presynaptic layer\n","            post_dimension (int): number of neurons in the postsynaptic layer\n","            weights (ndarray): connection weights of shape post_dimension x pre_dimension\n","\n","        This function is complete. You do not need to do anything here.\n","\n","        \"\"\"\n","        self.weights = weights\n","        self.pre_dimension = pre_dimension\n","        self.post_dimension = post_dimension\n","    \n","    def __call__(self, spike_input):\n","        \"\"\"\n","        Args:\n","            spike_input (ndarray): spikes generated by the pre-synaptic neurons\n","        Return:\n","            psp: current for the post-synaptic neurons\n","        \n","        Write the operation for computing psp\n","        \"\"\"\n","        \n","        #Compute psp given spike_input and self.weights\n","        psp = np.dot(self.weights, spike_input)\n","        \n","        return psp"]},{"cell_type":"markdown","metadata":{"id":"3D0CKo14hgxD"},"source":["To verify the correctness of your class implementation, create a connection object and compute the postsynaptic current for random presynaptic activation inputs and random connection weights. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0GgLyzAhgxD","outputId":"0370b520-1d9e-46b3-803c-f0c02f2fd1f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["current shape :  (50, 1)\n"]}],"source":["#Define the dimensions of the presynaptic layer in a variable\n","pre_dimension = 100\n","\n","#Define the dimensions of the postsynaptic layer in a variable\n","post_dimension = 50\n","\n","#Create random presynaptic inputs with any probability. Numpy random choice function might be useful here. \n","presynaptic_input_spike = np.random.choice([1,0], (pre_dimension,1), p=[0.6, 0.4])\n","\n","#Create a random connection weight matrix. Numpy random rand function might be useful here. \n","weight_matrix = np.random.rand(post_dimension, pre_dimension)\n","\n","#Initialize a connection object using the Connection class definition and pass the variables created above as arguments\n","connection =  Connections(weight_matrix, pre_dimension, post_dimension)\n","\n","#Compute the current for the postsynaptic layer when the connection object is fed random presynaptic activation inputs\n","psp = connection.__call__(presynaptic_input_spike)\n","\n","#Print the shape of the current\n","print(\"current shape : \", psp.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"n_O-mqFxhgxD"},"source":["# Question 3: Constructing Feedforward SNN [15 points]\n","Now that you have implemented the basic elements of an SNN- layer and connection, you are all set to implement a fully functioning SNN. The SNN that you will implement here consists of an input layer, a hidden layer, and an output layer. \n","\n","Below is the class definition of an SNN. Your task is to create the layers and connections that form the network using the class definitions in Question 2. Then complete the function to propagate a given input through the network and decode network output. \n","\n","## Rubric: 10 points for correct implementation of class. 5 points for the right verification. [15 points]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8-atf-NhgxD"},"outputs":[],"source":["class SNN:\n","    \"\"\" Define a Spiking Neural Network with One Hidden Layer \"\"\"\n","\n","    def __init__(self, input_2_hidden_weight, hidden_2_output_weight, \n","                 input_dimension=784, hidden_dimension=256, output_dimension=10,\n","                 vdecay=0.5, vth=0.5, snn_timestep=20):\n","        \"\"\"\n","        Args:\n","            input_2_hidden_weight (ndarray): weights for connection between input and hidden layer. dimension should be hidden_dimension x input_dimension. \n","            hidden_2_output_weight (ndarray): weights for connection between hidden and output layer. dimension should be output dimension x hidden dimension. \n","            input_dimension (int): number of neurons in the input layer\n","            hidden_dimension (int): number of neurons in the hidden layer\n","            output_dimension (int): number of neurons in the output layer\n","            vdecay (float): voltage decay of LIF neurons\n","            vth (float): voltage threshold of LIF neurons\n","            snn_timestep (int): number of timesteps for simulating the network (also called inference timesteps)\n","        \"\"\"\n","        self.snn_timestep = snn_timestep\n","        \n","        #Create the hidden layer\n","        self.hidden_layer = LIFNeurons(hidden_dimension, vdecay, vth)\n","        \n","        #Create the output layer\n","        self.output_layer = LIFNeurons(output_dimension, vdecay, vth)\n","        \n","        #Create the connection between input and hidden layer\n","        self.connection_input_to_hidden = Connections(input_2_hidden_weight,input_dimension,hidden_dimension )\n","        \n","        #Create the connection between hidden and output layer\n","        self.connection_hidden_to_output = Connections(hidden_2_output_weight,hidden_dimension,output_dimension)\n","        \n","    \n","    def __call__(self, spike_encoding):\n","        \"\"\"\n","        Args:\n","            spike_encoding (ndarray): spike encoding of input\n","        Return:\n","            output: decoded output from the network\n","        \"\"\"\n","        \n","        #Initialize an array to store the decoded network output for all neurons in the output layer\n","        spike_output = np.zeros(self.output_layer.dimension)\n","        \n","        #Loop through the simulation timesteps and process the input at each timestep tt\n","        for timestep in range(self.snn_timestep):\n","            \n","            #Propagate the input through the input to hidden layer and compute current for hidden layer\n","            hidden_current_in = self.connection_input_to_hidden.__call__(spike_encoding[timestep])\n","           \n","            #Compute hidden layer spikes \n","            hidden_spikes = self.hidden_layer.__call__(hidden_current_in)\n","            \n","            #Propagate hidden layer inputs to output layer and compute current for output layer\n","            output_current_in = self.connection_hidden_to_output.__call__(hidden_spikes)\n","            \n","            #compute output layer spikes\n","            output_spikes = self.output_layer.__call__(output_current_in)\n","            \n","            #Decode spike outputs by summing them up\n","            spike_output += output_spikes\n","            \n","        return spike_output"]},{"cell_type":"markdown","metadata":{"id":"HABiKcxLhgxD"},"source":["To verify the correctness of your class implementation, define the arguments to initialize the SNN. Then initialize the SNN and pass through it random inputs and compute network outputs. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62Z8LCurhgxD","outputId":"a922a74c-4f8a-4332-f064-0f933cbaf127"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Spikes of the nework  : \n"," [[0 1 1 ... 1 1 1]\n"," [1 0 0 ... 1 1 0]\n"," [1 1 1 ... 1 1 1]\n"," ...\n"," [1 0 1 ... 1 1 1]\n"," [0 1 1 ... 1 0 0]\n"," [0 0 0 ... 0 0 0]]\n","Output Spikes of the network : \n"," [20. 20. 20. 20. 20. 20. 20. 20. 20. 20.]\n"]}],"source":["#Define the dimensions of the input layer in a variable\n","input_dimension=784\n","\n","#Define the dimensions of the hidden layer in a variable\n","hidden_dimension = 256\n","\n","#Define the dimensions of the output layer in a variable\n","output_dimension=10\n","\n","#Define vdecay in a variable\n","vdecay=0.5\n","\n","#Define vth in a variable\n","vth=0.5\n","\n","#Define snn_timesteps in a variable\n","snn_timestep=20\n","\n","#Create random input to hidden layer weights. Numpy random rand function might be useful here\n","input_2_hidden_weight = np.random.rand(hidden_dimension, input_dimension)\n","\n","\n","#Create random hidden to output layer weights. Numpy random rand function might be useful here\n","hidden_2_output_weight = np.random.rand(output_dimension, hidden_dimension)\n","\n","\n","#Create random spike inputs to the network. Numpy random choice function might be useful here\n","spike_input = np.random.choice([1,0], (snn_timestep,input_dimension), p=[0.6, 0.4])\n","\n","\n","#Print the inputs\n","print(\"Input Spikes of the nework  : \\n\" , spike_input)\n","\n","\n","#Create an SNN object using the class definition and variables defined above\n","snn = SNN(input_2_hidden_weight, hidden_2_output_weight, input_dimension, hidden_dimension, output_dimension,\n","                 vdecay, vth, snn_timestep)\n","\n","#Pass the random spike inputs through the SNN and print the output of the SNN\n","output_spikes = snn.__call__(spike_input)\n","print(\"Output Spikes of the network : \\n\", output_spikes)"]},{"cell_type":"markdown","metadata":{"id":"e5ICWMMyhgxE"},"source":["# Question 4: SNN for Classification of Digits [25 points]\n","So far we have learnt how to construct SNNs for random inputs. In this exercise, you will use your implementation of SNNs to classify real-world data, taking the dataset of handwritten digits as an example. The dataset is provided as numpy arrays in the folder \"data\". Each sample in the MNIST dataset is a 28x28 image of a digit and a label (between 0 and 9) of that image. We will be dealing with batches, which means that we will read a fixed number of samples from the dataset (also called the batch size).\n","\n","## 4a. \n","First, we need to write two helper functions- to read the data from the saved data files, and to convert an image into spikes. The function to read the data is already written for you. You need to complete the function for encoding the data into spikes. \n","\n","## Rubric: 7 points for correct implementation of function. 3 points for the right verification. [10 points]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeRynSJKhgxE"},"outputs":[],"source":["def read_numpy_mnist_data(save_root, num_sample):\n","    \"\"\"\n","    Read saved numpy MNIST data\n","    Args:\n","        save_root (str): path to the folder where the MNIST data is saved\n","        num_sample (int): number of samples to read\n","    Returns:\n","        image_list: list of MNIST image\n","        label_list: list of corresponding labels\n","    \n","    This function is complete. You do not need to do anything here.\n","    \"\"\"\n","    image_list = np.zeros((num_sample, 28, 28))\n","    label_list = []\n","    for ii in range(num_sample):\n","        image_label = pickle.load(open(save_root + '/' + str(ii) + '.p', 'rb'))\n","        image_list[ii] = image_label[0]\n","        label_list.append(image_label[1])\n","\n","    return image_list, label_list\n","\n","def img_2_event_img(image, snn_timestep):\n","    \"\"\"\n","    Transform image to spikes, also called an event image\n","    Args:\n","        image (ndarray): image of shape batch_size x 28 x 28\n","        snn_timestep (int): spike timestep\n","    Returns:\n","        event_image: event image- spike encoding of the image\n","        \n","    Complete the expression for converting the image to spikes (event image)\n","    \"\"\"\n","    \n","    #Reshape the image. Do not touch this code\n","    batch_size = image.shape[0]\n","    image_size = image.shape[2]\n","    image = image.reshape(batch_size, image_size, image_size, 1)\n","    \n","    #Generate a random image of the shape batch_size x image_size x image_size x snn_timestep. Numpy random rand function will be useful here. \n","    rand_img = np.random.rand(batch_size, image_size, image_size, snn_timestep)\n","\n","    \n","    #Generate the event image\n","    event_image = (image > rand_img).astype(float)\n","    \n","\n","    return event_image"]},{"cell_type":"markdown","metadata":{"id":"FG8hhcX6hgxE"},"source":["To verify the correctness of your class implementation, load a sample digit from the saved file and convert it into an event image. Then print the shape of the event image. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPxgwrc4hgxE","outputId":"4a084f24-da4e-428c-c5c8-f2ec4edc1d7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["data shape :  (1000, 28, 28)\n","Shape of the event image:-  (1000, 28, 28, 20)\n"]}],"source":["#Load 1000 samples from the MNIST dataset using the read function defined above\n","images , labels = read_numpy_mnist_data(\"data/mnist_test\",1000)\n","\n","\n","#Print the shape of the data\n","print(\"data shape : \",images.shape)\n","\n","\n","#Convert the images to event images\n","event_images = img_2_event_img(images , snn_timestep)\n","\n","\n","#Print the shape of the event image\n","print(\"Shape of the event image:- \",event_images.shape)\n","\n"]},{"cell_type":"code","source":["\n","rand_img = np.random.rand(100, 28, 28, 20)"],"metadata":{"id":"mTxiLtAYweLe","executionInfo":{"status":"ok","timestamp":1682364845231,"user_tz":240,"elapsed":282,"user":{"displayName":"Pradeep Roy Yadlapalli","userId":"07982189018046624715"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["rand_img[0].reshape( -1 , rand_img.shape[-1]).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dANWhDVHwlHz","executionInfo":{"status":"ok","timestamp":1682364948123,"user_tz":240,"elapsed":45,"user":{"displayName":"Pradeep Roy Yadlapalli","userId":"07982189018046624715"}},"outputId":"732e4103-0b1d-4782-8d65-806878faf4a1"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(784, 20)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"0wuiWWHrhgxE"},"source":["## 4b. \n","Next, we need another helper function to compute the classification accuracy of the network. The classification accuracy is defined as the percentage of the samples that the network classifies correctly. To compute the classification accuracy, you need to:\n","\n","- Propagate each input through the network and obtain the network output.\n","- Based on the network output, the class of the image is the one for which the output neuron has maximum value. Let's call this predicted class. \n","- Compare the predicted class against the true class. \n","- Compute accuracy as the percentage of correct predictions. \n","\n","Below is the function for computing the test accuracy. The function takes in as arguments the SNN, directory in which the MNIST data is saved, and the number of samples to take from the MNIST dataset. Your task is to use the helper functions created above to load the data, convert into event images, and then compute network prediction and accuracies. \n","\n","## Rubric: 15 points for correct implementation of function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QyXqfa27hgxE"},"outputs":[],"source":["def test_snn_with_mnist(network, data_save_dir, data_sample_num):\n","    \"\"\"\n","    Test SNN with MNIST test data\n","    Args:\n","        network (SNN): defined SNN network\n","        data_save_dir (str): directory for the test data\n","        data_sample_num (int): number of test data examples\n","    \"\"\"\n","    #Read image and labels using the read function\n","    test_image_list, test_label_list = read_numpy_mnist_data(data_save_dir, data_sample_num)\n","    \n","    #Convert the images to event images\n","    test_event_image_list = img_2_event_img(test_image_list, network.snn_timestep)\n","    \n","    \n","    #Initialize number of correct predictions to 0\n","    correct_prediction = 0\n","    \n","    #Loop through the test images\n","    for i in range(data_sample_num):\n","        #Compute network output for each image. You might have to reshape the image using Numpy reshape function so that its appropriate for the SNN\n","        output = test_event_image_list[i].reshape( -1 , test_event_image_list.shape[-1])\n","        output = output.T # making time steps as rows and the remaining as features\n","        network_output = network.__call__(output)\n","        \n","        #Determine the class of the image from the network output. Numpy argmax function might be useful here\n","        pred_class = np.argmax(network_output)\n","        \n","        \n","        #Compare the predicted class against true class and update correct_prediction counter\n","        if pred_class == test_label_list[i]:\n","            correct_prediction +=1\n","        \n","    #Compute test accuracy\n","    test_accuracy = (correct_prediction/data_sample_num)*100\n","\n","    \n","    return test_accuracy"]},{"cell_type":"markdown","metadata":{"id":"V_NIY4fVhgxE"},"source":["# Question 5: Tuning Membrane Properties for Correct Classification [25 points]\n","Great! We have everything that we need to measure the performance of the SNN for classification of MNIST digits. For this, we first need to create the SNN using the class definition we wrote in Q.3. Then we need to call the test function that we wrote in Q.4b. However, note that the SNN needs the connection weights between the layers as inputs. These weights are typically obtained as a result of \"training\" the network for a given task (such as MNIST classification). However, since training the network isn't a part of this assignment, we provide to you already trained weights. \n","\n","## 5a. \n","Your task in this exercise is to initialize an SNN with vdecay=1.0 and vth=0.5. Test the SNN on MNIST dataset and obtain the classification accuracy.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O66Z1FjvhgxE","outputId":"0242f88a-7d0a-48a9-a46f-26140170d92a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy of MNIST :  24.4\n"]}],"source":["#Load the weights. Do not touch this code\n","snn_param_dir = 'save_models/snn_bptt_mnist_train.p'\n","snn_param_dict = pickle.load(open(snn_param_dir, 'rb'))\n","input_2_hidden_weight = snn_param_dict['weight1']\n","hidden_2_output_weight = snn_param_dict['weight2']\n","\n","#Define a variable for vdecay\n","vdecay = 1\n","\n","#Define a variable for vth\n","vth = 0.5\n","\n","#Create the SNN using the class definition in Q3 and the variables defined above\n","snn = SNN(input_2_hidden_weight, hidden_2_output_weight, input_dimension, hidden_dimension, output_dimension,\n","                 vdecay, vth, snn_timestep = 20)\n","\n","#Compute test accuracy for the SNN on 1000 examples from MNIST dataset and print it\n","test_accuracy  = test_snn_with_mnist(snn ,\"data/mnist_test\" , 1000 )\n","print(\"Test Accuracy of MNIST : \" , test_accuracy)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-l8oFchxhgxF"},"source":["What could be a possible reason for the poor accuracy? (Hint: Consider the hyperparameters, e.g. the neuron properties, and also the parameter, e.g. the connections between neurons)"]},{"cell_type":"markdown","metadata":{"id":"z-iCCk3dhgxF"},"source":["## Answer 5a. \n","\n","The Snn network model involes many hyperparameters like the voltage decay , current decay , resting potential , voltage threshold  etc . we need to tune these hyperparameters to identify the optimal values for attaining high accuracy\n","\n","\n","\n","## Rubric: 7 points for correct training. 3 points for the explanation. [10 points]"]},{"cell_type":"markdown","metadata":{"id":"FPSDCyClhgxF"},"source":["## 5b. \n","Can you tune the membrane properties (vdecay and vth) to obtain higher classification accuracies?\n","\n","## Rubric: 10 points if accuracy above 90%. 8 points if between 70 and 90%. 5 points if between 50 and 70%. 0 points otherwise. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8m1-Y-J2hgxF","outputId":"76d52ec3-d284-4a06-b0b3-63f282d7877b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy of MNIST :  97.3\n"]}],"source":["#Write your implementation of Question 5b. here\n","\n","#Load the weights. Do not touch this code\n","snn_param_dir = 'save_models/snn_bptt_mnist_train.p'\n","snn_param_dict = pickle.load(open(snn_param_dir, 'rb'))\n","input_2_hidden_weight = snn_param_dict['weight1']\n","hidden_2_output_weight = snn_param_dict['weight2']\n","\n","#Define a variable for vdecay\n","vdecay = 0.5\n","\n","#Define a variable for vth\n","vth = 0.5\n","\n","#Create the SNN using the class definition in Q3 and the variables defined above\n","snn = SNN(input_2_hidden_weight, hidden_2_output_weight, input_dimension, hidden_dimension, output_dimension,\n","                 vdecay, vth, snn_timestep = 20)\n","\n","#Compute test accuracy for the SNN on 1000 examples from MNIST dataset and print it\n","test_accuracy  = test_snn_with_mnist(snn ,\"data/mnist_test\" , 1000 )\n","print(\"Test Accuracy of MNIST : \" , test_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"gxvW5s16hgxF"},"source":["## 5c.\n","Based on your response to Questions 5a. and 5b., can you explain how membrane properties affect network activity for classification?"]},{"cell_type":"markdown","metadata":{"id":"hUSi1xrohgxF"},"source":["## Answer 5c.\n","\n","When the vdecay was high intially (Vdecay = 1) the accuracy was 24.4% but when vdecay reduced to 0.5 the accuracy significantly improved by a large margin attaining 97.3% .  \n","\n","\n","## Rubric: 3 points"]},{"cell_type":"markdown","metadata":{"id":"kMGMtmq3hgxF"},"source":["## 5d.\n","Besides the membrane properties, what other impoertant factors can affect the classification accuracy? How should we improve it? Hint: Remember here we use a feedford network."]},{"cell_type":"markdown","metadata":{"id":"GemvxJSchgxF"},"source":["## Answer 5d.\n","\n","1. Training data quality can be improved \n","2. Make our model robust by training on images which are highly variational( noise in data )\n","3. Network architecture can be improved.( more hidden layers)\n","4. Regularization techniques on the network( something similar to dropout to prevent model overfitting)\n","5. Further tuning the Vdecay and Vth parameters\n","\n","## Rubric: 2 points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_ZRT9WihgxF"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}